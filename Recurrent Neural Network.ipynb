{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "from random import randint\n",
    "import datetime\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())\n",
    "\n",
    "maxSeqLength = 25 #Maximum length of sentence\n",
    "numDimensions = 300 #Dimensions for each word vector\n",
    "    \n",
    "wordsList = np.load('wordsList.npy')\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load('wordVectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n",
      "The total number of files is 5121\n",
      "The total number of words in the files is 102607\n",
      "The average number of words in the files is 20.0365163054091\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYpBIkjoxSCRJnRgkkqRO/j+O0Vi82r9KwQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b931e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "positiveFiles = ['positive/' + f for f in listdir('positive/') if isfile(join('positive/', f))]\n",
    "negativeFiles = ['negative/' + f for f in listdir('negative/') if isfile(join('negative/', f))]\n",
    "numWords = []\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "minLength = len(positiveFiles)\n",
    "\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('Negative files finished')\n",
    "\n",
    "if (minLength > len(negativeFiles)):\n",
    "    minLength = len(negativeFiles)\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print('The total number of files is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))\n",
    "\n",
    "ids = np.zeros((minLength*2, maxSeqLength), dtype='int32')\n",
    "labels = []\n",
    "fileCounter = 0\n",
    "counter = 0\n",
    "for pf in positiveFiles:\n",
    "    if (counter >= minLength):\n",
    "        break\n",
    "    with open(pf, \"r\") as f:\n",
    "        indexCounter = 0\n",
    "        line=f.readline()\n",
    "        cleanedLine = cleanSentences(line)\n",
    "        split = cleanedLine.split()\n",
    "        for word in split:\n",
    "            try:\n",
    "                ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "            except ValueError:\n",
    "                ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "            indexCounter = indexCounter + 1\n",
    "            if indexCounter >= maxSeqLength:\n",
    "                break\n",
    "        labels.append([1,0])\n",
    "        fileCounter = fileCounter + 1\n",
    "        counter = counter + 1\n",
    "\n",
    "counter = 0\n",
    "for nf in negativeFiles:\n",
    "    if (counter > minLength):\n",
    "        break\n",
    "    with open(nf, \"r\") as f:\n",
    "        indexCounter = 0\n",
    "        line=f.readline()\n",
    "        cleanedLine = cleanSentences(line)\n",
    "        split = cleanedLine.split()\n",
    "        for word in split:\n",
    "            try:\n",
    "                ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "            except ValueError:\n",
    "                ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "            indexCounter = indexCounter + 1\n",
    "            if indexCounter >= maxSeqLength:\n",
    "                break\n",
    "        labels.append([0,1])\n",
    "        fileCounter = fileCounter + 1 \n",
    "        counter = counter + 1\n",
    "#Pass into embedding function and see if it evaluates. \n",
    "\n",
    "labels = np.asarray(labels)\n",
    "np.save('idsMatrix', ids)\n",
    "np.save('labels', labels)\n",
    "\n",
    "plt.hist(numWords, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 50, 0, 1000])\n",
    "plt.show()\n",
    "plt.savefig(\"figures/{}-histogram.png\".format(st), format='png')\n",
    "# generate log files here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = np.load('idsMatrix.npy')\n",
    "labels = np.load('labels.npy')\n",
    "\n",
    "ids, labels = shuffle(ids, labels, random_state = 0)\n",
    "lastTrainingIndex = int(round(len(ids) * .90))\n",
    "ids = np.split(ids, [lastTrainingIndex, len(ids)])\n",
    "labels = np.split(labels, [lastTrainingIndex, len(labels)])\n",
    "trainingLabels = labels[0]\n",
    "testingLabels = labels[1]\n",
    "trainingIds = ids[0]\n",
    "testingIds = ids[1]\n",
    "\n",
    "def getTrainBatch(counter):\n",
    "    label = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        arr[i] = trainingIds[counter-1:counter]\n",
    "        label.append(trainingLabels[counter])\n",
    "        if (counter >= len(trainingIds) - 1):\n",
    "            counter = 1\n",
    "        else:\n",
    "            counter = counter + 1\n",
    "    return arr, label, counter\n",
    "\n",
    "def getTestBatch(counter):\n",
    "    label = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        arr[i] = testingIds[counter-1:counter]\n",
    "        label.append(testingLabels[counter])\n",
    "        if (counter >= len(testingIds) - 1):\n",
    "            counter = 1\n",
    "        else:\n",
    "            counter = counter + 1\n",
    "    return arr, label, counter\n",
    "\n",
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "\n",
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)\n",
    "\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)\n",
    "\n",
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "# generate log files here\n",
    "ts = time.time()\n",
    "st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
    "runlog = open(\"logs/{}.txt\".format(st),\"w+\")\n",
    "runlog.write(\"Batch Size: {}\\n\".format(batchSize))\n",
    "runlog.write(\"Number of Classes: {}\\n\".format(numClasses))\n",
    "runlog.write(\"Training Iterations: {}\\n\".format(iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = 1\n",
    "for i in range(iterations):\n",
    "   #Next Batch of reviews\n",
    "   nextBatch, nextBatchLabels, counter = getTrainBatch(counter)\n",
    "   sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "   \n",
    "   #Write summary to Tensorboard\n",
    "   if (i % 50 == 0):\n",
    "       summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "       writer.add_summary(summary, i)\n",
    "\n",
    "   #Save the network every 10,000 training iterations\n",
    "   if (i % 10000 == 0 and i != 0):\n",
    "       save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "       print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/pretrained_lstm.ckpt-90000\n",
      "Accuracy for this batch: 62.5\n",
      "Accuracy for this batch: 33.3333343267\n",
      "Accuracy for this batch: 33.3333343267\n",
      "Accuracy for this batch: 25.0\n",
      "Accuracy for this batch: 37.5\n",
      "Accuracy for this batch: 45.8333343267\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 41.6666656733\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 37.5\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 70.8333313465\n",
      "Accuracy for this batch: 37.5\n",
      "Accuracy for this batch: 33.3333343267\n",
      "Accuracy for this batch: 33.3333343267\n",
      "Accuracy for this batch: 33.3333343267\n",
      "Accuracy for this batch: 29.1666656733\n",
      "Accuracy for this batch: 58.3333313465\n",
      "Accuracy for this batch: 45.8333343267\n",
      "Accuracy for this batch: 41.6666656733\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 45.8333343267\n",
      "Accuracy for this batch: 58.3333313465\n",
      "Accuracy for this batch: 58.3333313465\n",
      "Accuracy for this batch: 33.3333343267\n",
      "Accuracy for this batch: 37.5\n",
      "Accuracy for this batch: 25.0\n",
      "Accuracy for this batch: 37.5\n",
      "Accuracy for this batch: 41.6666656733\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 41.6666656733\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 37.5\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 70.8333313465\n",
      "Accuracy for this batch: 37.5\n",
      "Accuracy for this batch: 33.3333343267\n",
      "Accuracy for this batch: 33.3333343267\n",
      "Accuracy for this batch: 33.3333343267\n",
      "Accuracy for this batch: 29.1666656733\n",
      "Accuracy for this batch: 62.5\n",
      "Accuracy for this batch: 45.8333343267\n",
      "Accuracy for this batch: 41.6666656733\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 45.8333343267\n",
      "Accuracy for this batch: 62.5\n",
      "Accuracy for this batch: 58.3333313465\n",
      "Accuracy for this batch: 33.3333343267\n",
      "Accuracy for this batch: 37.5\n",
      "Accuracy for this batch: 29.1666656733\n",
      "Accuracy for this batch: 37.5\n",
      "Accuracy for this batch: 41.6666656733\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 41.6666656733\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 37.5\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 70.8333313465\n",
      "Accuracy for this batch: 37.5\n",
      "Accuracy for this batch: 33.3333343267\n",
      "Accuracy for this batch: 33.3333343267\n",
      "Accuracy for this batch: 33.3333343267\n",
      "Accuracy for this batch: 29.1666656733\n",
      "Accuracy for this batch: 62.5\n",
      "Accuracy for this batch: 45.8333343267\n",
      "Accuracy for this batch: 41.6666656733\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 45.8333343267\n",
      "Accuracy for this batch: 62.5\n",
      "Accuracy for this batch: 58.3333313465\n",
      "Accuracy for this batch: 33.3333343267\n",
      "Accuracy for this batch: 37.5\n",
      "Accuracy for this batch: 25.0\n",
      "Accuracy for this batch: 37.5\n",
      "Accuracy for this batch: 45.8333343267\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 41.6666656733\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 37.5\n",
      "Accuracy for this batch: 54.1666686535\n",
      "Accuracy for this batch: 70.8333313465\n",
      "Accuracy for this batch: 37.5\n",
      "Accuracy for this batch: 33.3333343267\n",
      "Accuracy for this batch: 33.3333343267\n",
      "Accuracy for this batch: 33.3333343267\n",
      "Accuracy for this batch: 29.1666656733\n",
      "Accuracy for this batch: 58.3333313465\n",
      "Accuracy for this batch: 45.8333343267\n",
      "Accuracy for this batch: 41.6666656733\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 45.8333343267\n",
      "Accuracy for this batch: 58.3333313465\n",
      "Accuracy for this batch: 58.3333313465\n",
      "Accuracy for this batch: 33.3333343267\n",
      "Accuracy for this batch: 37.5\n",
      "Accuracy for this batch: 25.0\n",
      "Accuracy for this batch: 37.5\n",
      "Accuracy for this batch: 45.8333343267\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 41.6666656733\n",
      "7 5\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess, tf.train.latest_checkpoint('models'))\n",
    "\n",
    "counter = 1\n",
    "\n",
    "iterations = 100\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels, counter = getTestBatch(counter)\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)\n",
    "    runlog.write(\"Accuracy for this batch: {}\\n\".format((sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100))\n",
    "\n",
    "pp = 0\n",
    "nn = 0\n",
    "npp = 0\n",
    "pn = 0\n",
    "\n",
    "nextBatch, nextBatchLabels, counter = getTestBatch(counter)\n",
    "predictions = sess.run(prediction, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "for index in range(len(predictions)):\n",
    "    positive_sentiment, negative_sentiment = predictions[index][0], predictions[index][1]\n",
    "    if (positive_sentiment > negative_sentiment):\n",
    "        if ([1,0] == nextBatchLabels[index].tolist()):\n",
    "            pp = pp + 1\n",
    "        else:\n",
    "            npp = npp + 1\n",
    "    else:\n",
    "        if ([0,1] == nextBatchLabels[index].tolist()):\n",
    "            nn = nn + 1\n",
    "        else:\n",
    "            pn = pn + 1\n",
    "\n",
    "print(pp, nn)\n",
    "\n",
    "runlog.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
